{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [激活函数](#激活函数)\n",
    "    - [Sigmoid](#Sigmoid)\n",
    "    - [tanh](#tanh)\n",
    "    - [SoftMax](#SoftMax)\n",
    "    - [ReLUs](#ReLU)\n",
    "- [反向传播](#反向传播)\n",
    "- [损失函数](#损失函数)\n",
    "- [学习优化](#学习优化)\n",
    "- [算法实现](#算法实现)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/neural_network.png)\n",
    "\n",
    "# 激活函数\n",
    "\n",
    "实质是选择输出形式，一般情况下\n",
    "\n",
    "- 连续值转为$(0,1)$概率输出选 sigmoid\n",
    "- $(0,1)$ 概率输出且和为 1，选 softmax\n",
    "- $(-1,1)$ 输出选 tanh\n",
    "- 单边抑制选 ReLUs\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/sigmoid.png)\n",
    "$$\n",
    "Sigmoid(x)=\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "也可以表征信号强度。用于控制门时，1 表示完全打开，0 表示关闭\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\mathrm{d\\sigma(x)}}{\\mathrm{d} x} &=\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left(\\frac{1}{1+e^{-x}}\\right) \\\\&=\\frac{\\mathrm{e}^{-x}}{\\left(1+\\mathrm{e}^{-x}\\right)^{2}} \\\\&=\\frac{e^{-x}}{1+e^{-x}}   \\frac{1}{1+e^{-x}} \n",
    "\\\\&=(1-\\frac{1}{1+e^{-x}} )\\frac{1}{1+e^{-x}} \n",
    "\\\\ &=\\sigma(1-\\sigma) \\end{aligned}\n",
    "$$\n",
    "\n",
    "## ReLU\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/ReLU.png)\n",
    "$$\n",
    "ReLU(x)=\\max(0,x)\n",
    "$$\n",
    "单边抑制性源自生物学。\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d} x} \\operatorname{ReLU}=\\left\\{\\begin{array}{ll}{1} & {x \\geqslant 0} \\\\ {0} & {x<0}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "## LeakyReLU\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/LeakyReLU.png)\n",
    "\n",
    "弥补 ReLU 在 $x<0$ 时会导致梯度消融的缺陷\n",
    "$$\n",
    "\\text { LeakyReLU } =\\left\\{\\begin{array}{ll}{x} & {x \\geqslant 0} \\\\ {p x} & {x<0}\\end{array}\\right.\n",
    "$$\n",
    "$p$ 是自行设置的较小超参数，比如 0.01 等。\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d} x} \\text { LeakyReLU }=\\left\\{\\begin{array}{ll}{1} & {x \\geqslant 0} \\\\ {p} & {x<0}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "## tanh\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/tanh.png)\n",
    "$$\n",
    "\\tanh (x)=\\frac{\\left(e^{x}-e^{-x}\\right)}{\\left(e^{x}+e^{-x}\\right)} =2 \\cdot \\operatorname{sigmoid}(2 x)-1\n",
    "$$\n",
    "控制输出和状态。可以视为 sigmoid 函数变换得到\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d} x} \\tanh (x)=& \\frac{\\left(e^{x}+e^{-x}\\right)\\left(e^{x}+e^{-x}\\right)-\\left(e^{x}-e^{-x}\\right)\\left(e^{x}-e^{-x}\\right)}{\\left(e^{x}+e^{-x}\\right)^{2}} \\\\ &=1-\\frac{\\left(e^{x}-e^{-x}\\right)^{2}}{\\left(e^{x}+e^{-x}\\right)^{2}}=1-\\tanh ^{2}(x) \\end{aligned}\n",
    "$$\n",
    "\n",
    "## SoftMax\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/softmax.png)\n",
    "\n",
    "求梯度\n",
    "$$\n",
    "\\frac{\\delta S_{i}}{\\delta x_{j}}=\\frac{\\delta \\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}}}{\\delta x_{j}}\n",
    "$$\n",
    "对$f(x) = \\frac{g(x)}{h(x)}$，有$f^{\\prime}(x)=\\frac{g^{\\prime}(x) h(x)-h^{\\prime}(x) g(x)}{[h(x)]^{2}}$，这里 $g_{i} =e^{x_{i}}, h_{i} =\\sum_{k=1}^{N} e^{x_{k}}$\n",
    "$$\n",
    "\\frac{\\delta g_{i}}{\\delta x_{j}}=\\left\\{\\begin{array}{ll}{e^{x_{j}},} & {\\text { if } i=j} \\\\ {0,} & {\\text { otherwise }}\\end{array}\\right.\\\\\n",
    "\\frac{\\delta h_{i}}{\\delta x_{j}}=\\frac{\\delta\\left(e^{x_{1}}+e^{x_{2}}+\\ldots+e^{x_{N}}\\right)}{\\delta x_{j}}=e^{x_{j}}\n",
    "$$\n",
    "\n",
    "- $i=j$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\delta \\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}}}{\\delta x_{j}} &=\\frac{e^{x_{j}}\\left(\\sum_{k=1}^{N} e^{x_{k}}-e^{{x_{i}}}\\right)}{\\left[\\sum_{k=1}^{N} e^{x_{k}}\\right]^{2}} \n",
    "\\\\ &=\\frac{e^{x_{j}}}{\\sum_{k=1}^{N} e^{x_{k}}} \\frac{\\sum_{k=1}^{N} e^{x_{k}}-e^{x_{i}} }{\\sum_{k=1}^{N} e^{x_{k}}}\n",
    "\\\\ &=\\frac{e^{x_{j}}}{\\sum_{k=1}^{N} e^{x_{k}}}\\left(\\frac{\\sum_{k=1}^{N} e^{x_{k}}}{\\sum_{k=1}^{N} e^{x_{k}}}-\\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}}\\right)\n",
    "\\\\ &=\\frac{e^{x_{j}}}{\\sum_{k=1}^{N} e^{x_{k}}}\\left(1-\\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}}\\right)^{x_{i}} \n",
    "\\\\ &=\\sigma\\left(x_{j}\\right)\\left(1-\\sigma\\left(x_{i}\\right)\\right)\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $i \\neq j$\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\frac{\\delta \\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}}}{\\delta x_{j}} &=\\frac{0-e^{x_{j}} e^{x_{i}}}{\\left[\\sum_{k=1}^{N} e^{x_{k}}\\right]^{2}} \\\\ &=0-\\frac{e^{x_{j}}}{\\sum_{k=1}^{N} e^{x_{k}}} \\frac{e^{x_{i}}}{\\sum_{k=1}^{N} e^{x_{k}}} \\\\ &=-\\sigma\\left(x_{j}\\right) \\sigma\\left(x_{i}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "矩阵形式\n",
    "$$\n",
    "\\frac{\\delta S}{\\delta x}=\\left[\\begin{array}{ccc}{\\frac{\\delta S_{1}}{\\delta x_{1}}} & {\\cdots} & {\\frac{\\delta S_{1}}{\\delta x_{N}}} \\\\ {\\ldots} & {\\frac{\\delta S_{i}}{\\delta x_{j}}} & {\\cdots} \\\\ {\\frac{\\delta S_{N}}{\\delta x_{1}}} & {\\cdots} & {\\frac{\\delta S_{N}}{\\delta x_{N}}}\\end{array}\\right]\\\\\n",
    "\\frac{\\delta S}{\\delta x}=\\left[\\begin{array}{ccc}{\\sigma\\left(x_{1}\\right)-\\sigma\\left(x_{1}\\right) \\sigma\\left(x_{1}\\right)} & {\\dots} & {0-\\sigma\\left(x_{1}\\right) \\sigma\\left(x_{N}\\right)} \\\\ {\\cdots} & {\\sigma\\left(x_{j}\\right)-\\sigma\\left(x_{j}\\right) \\sigma\\left(x_{i}\\right)} & {\\cdots} \\\\ {0-\\sigma\\left(x_{N}\\right) \\sigma\\left(x_{1}\\right)} & {\\cdots} & {\\sigma\\left(x_{N}\\right)-\\sigma\\left(x_{N}\\right) \\sigma\\left(x_{N}\\right)}\\end{array}\\right]\\\\\n",
    "\\frac{\\delta S}{\\delta x}=\\left[\\begin{array}{ccc}{\\sigma\\left(x_{1}\\right)} & {\\dots} & {0} \\\\ {\\dots} & {\\sigma\\left(x_{j}\\right)} & {\\dots} \\\\ {0} & {\\dots} & {\\sigma\\left(x_{N}\\right)}\\end{array}\\right]-\\left[\\begin{array}{ccc}{\\sigma\\left(x_{1}\\right) \\sigma\\left(x_{1}\\right)} & {\\dots} & {\\sigma\\left(x_{1}\\right) \\sigma\\left(x_{N}\\right)} \\\\ {\\cdots} & {\\sigma\\left(x_{j}\\right) \\sigma\\left(x_{i}\\right)} & {\\cdots} \\\\ {\\sigma\\left(x_{N}\\right) \\sigma\\left(x_{1}\\right)} & {\\dots} & {\\sigma\\left(x_{N}\\right) \\sigma\\left(x_{N}\\right)}\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "# 反向传播\n",
    "\n",
    "$w^l_{jk}$ 表示$(l-1)$层第 $j$ 个输入到 $l$ 层第 $k$ 个输出的权重\n",
    "\n",
    "$b^l_{j}$ 表示$l$层偏置\n",
    "\n",
    "$z^l_{j}$ 表示 $l$ 层第 $j$ 个神经元的输入：$z_{j}^{l}=\\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}$\n",
    "\n",
    "$a^l_j$ 表示 $l$ 层第 $j$ 个神经元的输出：$a_{j}^{l}=\\sigma\\left(\\sum_{k} w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\\right)$\n",
    "\n",
    "$\\sigma$ 表示激活函数\n",
    "\n",
    "$C$ 表示损失函数\n",
    "\n",
    "$L$ 为神经网络最大层数\n",
    "\n",
    "$l$ 层第 $l$ 个神经元的误差为 $\\delta_{j}^{l}=\\frac{\\partial C}{\\partial z_{j}^{l}}$\n",
    "\n",
    "- **最后一层**\n",
    "\n",
    "$$\n",
    "\\delta^{L}=\\nabla_{a} C \\odot \\sigma^{\\prime}\\left(z^{L}\\right)\\\\\n",
    "\\begin{array}{l}{\\because \\delta_{j}^{L}=\\frac{\\partial C}{\\partial z_{j}^{L}}=\\frac{\\partial C}{\\partial a_{j}^{L}} \\cdot \\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}=\\frac{\\partial C}{\\partial a_{j}^{L}} \\cdot \\sigma^{\\prime}\\left(z_{j}^{L}\\right)} \\\\ {\\therefore \\delta^{L}=\\frac{\\partial C}{\\partial a^{L}} \\odot \\frac{\\partial a^{L}}{\\partial z^{L}}=\\nabla_{a} C \\odot \\sigma^{\\prime}\\left(z^{L}\\right)}\\end{array}\n",
    "$$\n",
    "\n",
    "$\\odot$ 表示 Hadamard 乘积，对应项元素相乘\n",
    "\n",
    "- **中间任意两层间的损失**\n",
    "\n",
    "$$\n",
    "\\delta^{l}=\\left(\\left(w^{l+1}\\right)^{T} \\delta^{l+1}\\right) \\odot \\sigma^{\\prime}\\left(z^{l}\\right)\\\\\n",
    "\\begin{aligned} \\delta_{j}^{l}=\\frac{\\partial C}{\\partial z_{j}^{l}} &=\\sum_{k} \\frac{\\partial C}{\\partial z_{k}^{l+1}} \\cdot \\frac{\\partial z_{k}^{l+1}}{\\partial a_{j}^{l}} \\cdot \\frac{\\partial a_{j}^{l}}{\\partial z_{j}^{l}} \\\\ &=\\sum_{k} \\delta_{k}^{l+1} \\cdot \\frac{\\partial\\left(w_{k j}^{l+1} a_{j}^{l}+b_{k}^{l+1}\\right)}{\\partial a_{j}^{l}} \\cdot \\sigma^{\\prime}\\left(z_{j}^{l}\\right) \\\\ &=\\sum_{k} \\delta_{k}^{l+1} \\cdot w_{k j}^{l+1} \\cdot \\sigma^{\\prime}\\left(z_{j}^{l}\\right) \\end{aligned}\n",
    "$$\n",
    "\n",
    "- **权重梯度**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_{j k}^{l}}=a_{k}^{l-1} \\delta_{j}^{l}\\\\\n",
    "\\frac{\\partial C}{\\partial w_{j k}^{l}}=\\frac{\\partial C}{\\partial z_{j}^{l}} \\cdot \\frac{\\partial z_{j}^{l}}{\\partial w_{j k}^{l}}=\\delta_{j}^{l} \\cdot \\frac{\\partial\\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\\right)}{\\partial w_{j k}^{l}}=a_{k}^{l-1} \\delta_{j}^{l}\n",
    "$$\n",
    "\n",
    "- **偏置梯度**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial b_{j}^{l}}=\\delta_{j}^{l}\\\\\n",
    "\\frac{\\partial C}{\\partial b_{j}^{l}}=\\frac{\\partial C}{\\partial z_{j}^{l}} \\cdot \\frac{\\partial z_{j}^{l}}{\\partial b_{j}^{l}}=\\delta_{j}^{l} \\cdot \\frac{\\partial\\left(w_{j k}^{l} a_{k}^{l-1}+b_{j}^{l}\\right)}{\\partial b_{j}^{l}}=\\delta_{j}^{l}\n",
    "$$\n",
    "\n",
    "# 损失函数\n",
    "\n",
    "## 均方误差（MSE）\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\frac{1}{2} \\sum_{k=1}^{K}\\left(y_{k}-o_{k}\\right)^{2}\n",
    "$$\n",
    "\n",
    "$o_k$ 为预测输出值，$\\frac{1}{2}$作为常数方便求解梯度\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial o_{i}}&=\\frac{1}{2} \\sum_{k=1}^{K} \\frac{\\partial}{\\partial o_{i}}\\left(y_{k}-o_{k}\\right)^{2}\\\\\n",
    "&=\\frac{1}{2} \\sum_{k=1}^{K} 2 \\cdot\\left(y_{k}-o_{k}\\right) \\cdot \\frac{\\partial\\left(y_{k}-o_{k}\\right)}{\\partial o_{i}}\\\\\n",
    " &=\\sum_{k=1}^{K}\\left(y_{k}-o_{k}\\right) \\cdot(-1) \\cdot \\frac{\\partial o_{k}}{\\partial o_{i}} \\\\ &=\\sum_{k=1}^{K}\\left(o_{k}-y_{k}\\right) \\cdot \\frac{\\partial o_{k}}{\\partial o_{i}} \\end{aligned}\n",
    "$$\n",
    "当且仅当 $k = i$ 时 $\\frac{\\partial o_{k}}{\\partial o_{i}}=1$，即结点误差只与结点本身相关。\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial o_{i}}=o_i-y_i\n",
    "$$\n",
    "\n",
    "## 交叉熵损失\n",
    "\n",
    "$$\n",
    "H(p \\| q)=-\\sum_{i} p(i) \\log _{2} q(i) = H(p)+D_{K L}(p \\| q)\\\\\n",
    "D_{K L}(p \\| q)=\\sum_{i} p(i) \\log \\left(\\frac{p(i)}{q(i)}\\right)\n",
    "$$\n",
    "\n",
    "对分类问题，经 one-hot 编码，$H(p)=0$，进而有\n",
    "$$\n",
    "H(p \\| q)=D_{K L}(p \\| q) =\\sum_{j} y_{j} \\log \\left(\\frac{y_{j}}{o_{j}}\\right) =1 \\cdot \\log \\frac{1}{o_{i}}+\\sum_{j \\neq i} 0 \\cdot \\log \\left(\\frac{0}{o_{j}}\\right) =-\\log o_{i}\n",
    "$$\n",
    "由极大似然估计可得对数交叉熵损失为\n",
    "$$\n",
    "\\mathcal{L}=-\\sum_{k} y_{k} \\log \\left(p_{k}\\right)\n",
    "$$\n",
    "考虑对自变量 $z_i$ 求偏导\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_{i}}=-\\sum_{k} y_{k} \\frac{\\partial \\log \\left(p_{k}\\right)}{\\partial z_{i}}=-\\sum_{k} y_{k} \\frac{\\partial \\log \\left(p_{k}\\right)}{\\partial p_{k}} \\cdot \\frac{\\partial p_{k}}{\\partial z_{i}}=-\\sum_{k} y_{k} \\frac{1}{p_{k}} \\cdot \\frac{\\partial p_{k}}{\\partial z_{i}}\n",
    "$$\n",
    "输出分布 $p$ 为 SoftMax 时，将求和展开成两类情况的和\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_{i}}=-y_{i}\\left(1-p_{i}\\right)-\\sum_{k \\neq i} y_{k} \\frac{1}{p_{k}}\\left(-p_{k} \\cdot p_{i}\\right)=p_{i}\\left(y_{i}+\\sum_{k \\neq i} y_{k}\\right)-y_{i}\n",
    "$$\n",
    "对分类情况而言 $\\sum_{k} y_{k}=1,y_{i}+\\sum_{k \\neq i} y_{k}=1$，此时\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_{i}}=p_i-y_i\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习优化\n",
    "\n",
    "## 挑战\n",
    "\n",
    "- 病态。特别是海森矩阵。随机梯度下降会‘‘卡’’ 在某些情况，此时即使很小的更新步长也会增加代价函数。\n",
    "\n",
    "- 局部极小值。数量众多，代价不定\n",
    "- 高原、鞍点与平坦区域\n",
    "- 梯度爆炸和梯度消融\n",
    "- 悬崖\n",
    "\n",
    "## 基本算法\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_sgd.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_momentum_sgd.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_nestrove_sgd.png)\n",
    "\n",
    "Nesterov 动量中，梯度计算在施加当前速度之后。因此，Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。\n",
    "\n",
    "## 自适应学习率\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_adagrad.png)\n",
    "\n",
    "经验上，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_RMSprop.png)\n",
    "\n",
    "修改 AdaGrad 以在非凸情景下效果更好，改变梯度积累为指数加权的移动平均。\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_nestrove_RMSProp.png)\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-19_adam.png)\n",
    "\n",
    "## 批正则化\n",
    "\n",
    "应用于全连接/卷积层之后，非线性层（激活）之前。加速学习，降低初始依赖。\n",
    "$$\n",
    "x_{i} \\longleftarrow \\gamma \\frac{x_{i}-\\mu_{B}}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}+\\beta\n",
    "$$\n",
    " $\\epsilon$ 是很小的常数防止除零。$\\gamma,\\beta$ 是超参数保证变换后的 $x_i$ 能具有任意标准差与均值，此时的均值只与 $\\beta$ 有关，之前的均值还和其他时步关联，变换后更容易用 SGD 学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法实现\n",
    "\n",
    "- 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 硬件与版本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.7.3\n",
      "IPython 7.6.1\n",
      "\n",
      "ipywidgets 7.5.0\n",
      "numpy 1.16.4\n",
      "\n",
      "compiler   : MSC v.1915 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 60 Stepping 3, GenuineIntel\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -m -v -p ipywidgets,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 辅助函数设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(X, y=None, batch_size=64):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    for i in np.arrange(0, n_samples, batch_size):\n",
    "        begin, end = i, min(i+batch_size, n_samples)\n",
    "        if y is not None:\n",
    "            yield X[begin:end], y[begin:end]\n",
    "        else:\n",
    "            yield X[begin:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def loss(self, y, pred):\n",
    "        p = np.clip(p, 1e-15, 1-1e15)\n",
    "        \n",
    "        return -y * np.log(p) - (1-y) * np.log(1-p)\n",
    "    \n",
    "    def acc(self, y, pred):\n",
    "        return np.sum(y==pred, axis=0) / len(y)\n",
    "    \n",
    "    def gradient(self, y, pred):\n",
    "        p  = np.clip(p, 1e-15, 1-1e15)\n",
    "        return - (y / p) + (1 - y) / (1-p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(object):\n",
    "    def __init__(self, learning_rate=0.01, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Eg = None\n",
    "        self.eps = 1e-8\n",
    "        self.rho = rho\n",
    "    \n",
    "    def update(self, w, grad_w):\n",
    "        \n",
    "        if self.Eg is None:\n",
    "            self.Eg = np.zeros(np.shape(grad_w))\n",
    "            \n",
    "        self.Eg = self.rho * self.Eg + (1 - self.rho) * np.power(grad_w, 2)\n",
    "        \n",
    "        return w - self.learning_rate * grad_w / np.sqrt(self.Eg + self.eps)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense 层实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "    '''全连接层\n",
    "    \n",
    "    parameters:\n",
    "    ---------------\n",
    "    n_units: int\n",
    "        该层神经元数量\n",
    "    input_shape: tuple\n",
    "        该层期望接收的输入规格\n",
    "    '''\n",
    "    def __init__(self, n_units, input_shape=None):\n",
    "        self.layer_input = None\n",
    "        self.input_shape = input_shape\n",
    "        self.n_units = n_units\n",
    "        self.trainable = True\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "    \n",
    "    def set_input_shape(self, shape):\n",
    "        self.input_shape = shape\n",
    "        \n",
    "    def initialize(self, optimizer):\n",
    "        limit = 1 / math.sqrt(self.input_shape[0])\n",
    "        self.W  =np.random.uniform(-limit, limit, (self.input_shape[0], self.n_units))\n",
    "        self.b = np.zeros((1,self.n_units))\n",
    "        \n",
    "        self.W_opt = copy.copy(optimizer)\n",
    "        self.b_opt = copy.copy(optimizer)\n",
    "        \n",
    "    def layer_name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def parameters(self):\n",
    "        return np.prod(self.W.shape) + np.prod(self.b.shape)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        self.layer_input = X\n",
    "        return X.dot(self.W) + self.b  # 无激活函数\n",
    "    \n",
    "    def backward(self, accum_grad):\n",
    "        \n",
    "        W = self.W\n",
    "        \n",
    "        if self.trainable:\n",
    "            grad_w = self.layer_input.T.dot(accum_grad)\n",
    "            grad_b = np.sum(accum_grad, axis=0, keepdims=True)\n",
    "            \n",
    "            self.W = self.W_opt.update(self.W, grad_w)\n",
    "            self.b = self.b_opt.update(self.b, grad_b)\n",
    "        \n",
    "        accum_grad = accum_grad.T.dot(W.T)\n",
    "        return accum_grad\n",
    "    \n",
    "    def output_shape(self):\n",
    "        return (self.n_units,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''全连接神经网络\n",
    "    \n",
    "    parameters：\n",
    "    ----------------\n",
    "    optimizer: class\n",
    "        最小化损失时的优化器\n",
    "    loss：class\n",
    "        度量性能的损失函数。这里是交叉熵损失\n",
    "    validation: tuple\n",
    "        验证集（X,y）\n",
    "    '''\n",
    "    def __init__(self, optimizer, loss, validation_data=None):\n",
    "        self.optimizer = optimizer\n",
    "        self.layers = []\n",
    "        self.errors = {'training':[],'validation':[]}\n",
    "        self.loss = loss()\n",
    "        \n",
    "        self.val_set = None\n",
    "        if validation_data is not None:\n",
    "            X, y = validation_data\n",
    "            self.val_set = {'X':X, 'y':y}\n",
    "    \n",
    "    def set_trainable(self, trainable):\n",
    "        '''设置是否更新层中参数'''\n",
    "        for layer in self.layers:\n",
    "            layer.trainable = trainable\n",
    "            \n",
    "    def add(self, layer):\n",
    "        \n",
    "        if self.layers:\n",
    "            layer.set_input_shape(shape = self.layers[-1].output_shape())\n",
    "            \n",
    "        if hasattr(layer, 'initialize'):\n",
    "            layer.initialize(optimizer = self.optimizer)\n",
    "            \n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def test_on_batch(self, X, y):\n",
    "        y_pred = self._forward(X, training=False)\n",
    "        loss = np.mean(self.loss.loss(y, y_pred))\n",
    "        acc = self.loss.acc(y, y_pred)\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "    def train_on_batch(self, X, y):\n",
    "        y_pred = self._forward(X)\n",
    "        loss = np.mean(self.loss.loss(y, y_pred))\n",
    "        acc = self.loss.acc(y, y_pred)\n",
    "        loss_grad = self.loss.gradient(y, y_pred)\n",
    "        \n",
    "        self._backward(loss_grad=loss_grad)\n",
    "        \n",
    "        return loss, acc\n",
    "    \n",
    "    def fit(self, X_train, y_train, n_epochs, batch_size):\n",
    "        \n",
    "        for _ in range(n_epochs):\n",
    "            \n",
    "            batch_error = []\n",
    "            for X_batch, y_batch in batch_iterator(X_train, y_train, batch_size):\n",
    "                loss, _ = self.train_on_batch(X_batch, y_batch)\n",
    "                batch_error.append(loss)\n",
    "            \n",
    "            self.errors['training'].append(np.mean(batch_error))\n",
    "            \n",
    "            if self.val_set is not None:\n",
    "                val_loss, _ = self.test_on_batch(self.val_set['X'], self.val_set['y'])\n",
    "                self.errors['validation'].append(val_loss)\n",
    "            \n",
    "        return self.errors['training'], self.errors['validation']\n",
    "    \n",
    "    def _forward(self, X, training=True):\n",
    "        layer_output = X\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer.forward(layer_output, training)\n",
    "        \n",
    "        return layer_output\n",
    "    \n",
    "    def _backward(self, loss_grad):\n",
    "        for layer in self.layers:\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        return self._forward(X_test, training=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**作者**：Daniel Meng\n",
    "\n",
    "**GitHub**: [LibertyDream](https://github.com/LibertyDream)\n",
    "\n",
    "**博客**：[明月轩](https://libertydream.github.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
