{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [基本操作](#基本操作)\n",
    "    - [1.清理与替换](#1.-清理与替换)\n",
    "    - [2.截取](#2.-截取)\n",
    "    - [3.连接与分割](#3.-连接与分割)\n",
    "    - [4.比较与排序](#4.-比较与排序)\n",
    "    - [5.查找与包含（内置）](#5.-查找与包含（内置）)\n",
    "    - [6.大小写及其他变化](#6.-大小写与其他变化)\n",
    "- [模式匹配与正则表达式](#模式匹配与正则表达式)\n",
    "- [英文文本处理与NLTK](#英文文本处理与NLTK)\n",
    "    - [1. 分词（Tokenizaiton）](#1.-分词（Tokenizaiton）)\n",
    "    - [2. 词性标注](#2.-词性标注)\n",
    "    - [3. 组块分析（chunking）](#3.-组块分析（chunking）)\n",
    "    - [4. 命名实体识别](#4.-命名实体识别)\n",
    "    - [5. Stemming和Lemmatizing](#5.-Stemming和Lemmatizing)\n",
    "    - [6. WordNet 与词义解析](#6.-WordNet-与词义解析)\n",
    "- [英文文本处理与spaCy](#英文文本处理与spaCy)\n",
    "    - [1.Tokenizaiton](#1.Tokenization)\n",
    "    - [2.词性标注](#2.词性标注)\n",
    "    - [3.命名实体识别](#3.命名实体识别)\n",
    "    - [4.chunking](#4.chunking)\n",
    "    - [5.句法依存解析](#5.句法依存解析)\n",
    "    - [6.词向量](#6.词向量)\n",
    "    - [7.词汇与文本相似度](#7.词汇与文本相似度)\n",
    "- [中文文本处理与jieba](#中文文本处理与jieba)\n",
    "    - [1.分词](#1.分词)\n",
    "    - [2.jieba词性标注](#2.jieba词性标注)\n",
    "    - [3.关键词抽取](#3.关键词抽取)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本操作\n",
    "\n",
    "### 1. 清理与替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_str = ' hello world, hello, my name is Daniel Meng! '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 去除空格与特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world, hello, my name is Daniel Meng!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_str.strip().lstrip().rstrip(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 字符串替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi world, hi, my name is Daniel Meng! '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_str.replace('hello', 'hi')  # 非 in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_str = ' 大家好， 我是一轩明月！ '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家好， 我是一轩明月！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_str.strip().lstrip().rstrip(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家好， 我是小骡汤米！'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_str.strip().replace('一轩明月', '小骡汤米')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'， 我是一轩明月！'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_str.strip().replace('大家好', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 截取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_out_str = '大家好，我是一轩明月，信息技术爱好者，认识一下？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家好'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是一轩明月'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'认识一下'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家好，我是一轩明月，信息技术爱好者'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[:-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大好我一明，息术好，识下'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 翻转字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'？下一识认，者好爱术技息信，月明轩一是我，好家大'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_out_str[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 连接与分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_str1 = '大家好！'\n",
    "connect_str2 = '我是一轩明月'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家好！我是一轩明月'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect_str1 + connect_str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_strs = ['赤','橙','黄','绿','青','蓝','紫']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'赤;橙;黄;绿;青;蓝;紫'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "';'.join(connect_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赤|橙|黄|绿|青|蓝|紫\n",
      "['赤', '橙', '黄', '绿', '青', '蓝', '紫']\n"
     ]
    }
   ],
   "source": [
    "tmp_str = '|'.join(connect_strs)\n",
    "print(tmp_str)\n",
    "tmp_str = tmp_str.split('|')\n",
    "print(tmp_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 比较与排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_strs = ['xYz','AAb','cCe','aBd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAb', 'aBd', 'cCe', 'xYz']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cmp_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAb', 'aBd', 'cCe', 'xYz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(cmp_strs, key=lambda x:x[2].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 查找与包含（内置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_str = '举杯邀明月，对影成三人。月有阴晴圆缺，此事古难全。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_str.index('明月')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_str.find('月')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-faa11e0a97f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'错误'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "#search_str.index('错误')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_str.find('错误')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 大小写与其他变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_str = 'hello, my name is Daniel Meng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, my name is daniel meng'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO, MY NAME IS DANIEL MENG'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, my name is daniel meng'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class str in module builtins:\n",
      "\n",
      "class str(object)\n",
      " |  str(object='') -> str\n",
      " |  str(bytes_or_buffer[, encoding[, errors]]) -> str\n",
      " |  \n",
      " |  Create a new string object from the given object. If encoding or\n",
      " |  errors is specified, then the object must expose a data buffer\n",
      " |  that will be decoded using the given encoding and error handler.\n",
      " |  Otherwise, returns the result of object.__str__() (if defined)\n",
      " |  or repr(object).\n",
      " |  encoding defaults to sys.getdefaultencoding().\n",
      " |  errors defaults to 'strict'.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, value, /)\n",
      " |      Return self+value.\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      Return key in self.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(self, format_spec, /)\n",
      " |      Return a formatted version of the string as described by format_spec.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __getnewargs__(...)\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __mod__(self, value, /)\n",
      " |      Return self%value.\n",
      " |  \n",
      " |  __mul__(self, value, /)\n",
      " |      Return self*value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rmod__(self, value, /)\n",
      " |      Return value%self.\n",
      " |  \n",
      " |  __rmul__(self, value, /)\n",
      " |      Return value*self.\n",
      " |  \n",
      " |  __sizeof__(self, /)\n",
      " |      Return the size of the string in memory, in bytes.\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  capitalize(self, /)\n",
      " |      Return a capitalized version of the string.\n",
      " |      \n",
      " |      More specifically, make the first character have upper case and the rest lower\n",
      " |      case.\n",
      " |  \n",
      " |  casefold(self, /)\n",
      " |      Return a version of the string suitable for caseless comparisons.\n",
      " |  \n",
      " |  center(self, width, fillchar=' ', /)\n",
      " |      Return a centered string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  count(...)\n",
      " |      S.count(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the number of non-overlapping occurrences of substring sub in\n",
      " |      string S[start:end].  Optional arguments start and end are\n",
      " |      interpreted as in slice notation.\n",
      " |  \n",
      " |  encode(self, /, encoding='utf-8', errors='strict')\n",
      " |      Encode the string using the codec registered for encoding.\n",
      " |      \n",
      " |      encoding\n",
      " |        The encoding in which to encode the string.\n",
      " |      errors\n",
      " |        The error handling scheme to use for encoding errors.\n",
      " |        The default is 'strict' meaning that encoding errors raise a\n",
      " |        UnicodeEncodeError.  Other possible values are 'ignore', 'replace' and\n",
      " |        'xmlcharrefreplace' as well as any other name registered with\n",
      " |        codecs.register_error that can handle UnicodeEncodeErrors.\n",
      " |  \n",
      " |  endswith(...)\n",
      " |      S.endswith(suffix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S ends with the specified suffix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      suffix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  expandtabs(self, /, tabsize=8)\n",
      " |      Return a copy where all tab characters are expanded using spaces.\n",
      " |      \n",
      " |      If tabsize is not given, a tab size of 8 characters is assumed.\n",
      " |  \n",
      " |  find(...)\n",
      " |      S.find(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  format(...)\n",
      " |      S.format(*args, **kwargs) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from args and kwargs.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  format_map(...)\n",
      " |      S.format_map(mapping) -> str\n",
      " |      \n",
      " |      Return a formatted version of S, using substitutions from mapping.\n",
      " |      The substitutions are identified by braces ('{' and '}').\n",
      " |  \n",
      " |  index(...)\n",
      " |      S.index(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the lowest index in S where substring sub is found, \n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  isalnum(self, /)\n",
      " |      Return True if the string is an alpha-numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isalpha(self, /)\n",
      " |      Return True if the string is an alphabetic string, False otherwise.\n",
      " |      \n",
      " |      A string is alphabetic if all characters in the string are alphabetic and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isascii(self, /)\n",
      " |      Return True if all characters in the string are ASCII, False otherwise.\n",
      " |      \n",
      " |      ASCII characters have code points in the range U+0000-U+007F.\n",
      " |      Empty string is ASCII too.\n",
      " |  \n",
      " |  isdecimal(self, /)\n",
      " |      Return True if the string is a decimal string, False otherwise.\n",
      " |      \n",
      " |      A string is a decimal string if all characters in the string are decimal and\n",
      " |      there is at least one character in the string.\n",
      " |  \n",
      " |  isdigit(self, /)\n",
      " |      Return True if the string is a digit string, False otherwise.\n",
      " |      \n",
      " |      A string is a digit string if all characters in the string are digits and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  isidentifier(self, /)\n",
      " |      Return True if the string is a valid Python identifier, False otherwise.\n",
      " |      \n",
      " |      Use keyword.iskeyword() to test for reserved identifiers such as \"def\" and\n",
      " |      \"class\".\n",
      " |  \n",
      " |  islower(self, /)\n",
      " |      Return True if the string is a lowercase string, False otherwise.\n",
      " |      \n",
      " |      A string is lowercase if all cased characters in the string are lowercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  isnumeric(self, /)\n",
      " |      Return True if the string is a numeric string, False otherwise.\n",
      " |      \n",
      " |      A string is numeric if all characters in the string are numeric and there is at\n",
      " |      least one character in the string.\n",
      " |  \n",
      " |  isprintable(self, /)\n",
      " |      Return True if the string is printable, False otherwise.\n",
      " |      \n",
      " |      A string is printable if all of its characters are considered printable in\n",
      " |      repr() or if it is empty.\n",
      " |  \n",
      " |  isspace(self, /)\n",
      " |      Return True if the string is a whitespace string, False otherwise.\n",
      " |      \n",
      " |      A string is whitespace if all characters in the string are whitespace and there\n",
      " |      is at least one character in the string.\n",
      " |  \n",
      " |  istitle(self, /)\n",
      " |      Return True if the string is a title-cased string, False otherwise.\n",
      " |      \n",
      " |      In a title-cased string, upper- and title-case characters may only\n",
      " |      follow uncased characters and lowercase characters only cased ones.\n",
      " |  \n",
      " |  isupper(self, /)\n",
      " |      Return True if the string is an uppercase string, False otherwise.\n",
      " |      \n",
      " |      A string is uppercase if all cased characters in the string are uppercase and\n",
      " |      there is at least one cased character in the string.\n",
      " |  \n",
      " |  join(self, iterable, /)\n",
      " |      Concatenate any number of strings.\n",
      " |      \n",
      " |      The string whose method is called is inserted in between each given string.\n",
      " |      The result is returned as a new string.\n",
      " |      \n",
      " |      Example: '.'.join(['ab', 'pq', 'rs']) -> 'ab.pq.rs'\n",
      " |  \n",
      " |  ljust(self, width, fillchar=' ', /)\n",
      " |      Return a left-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  lower(self, /)\n",
      " |      Return a copy of the string converted to lowercase.\n",
      " |  \n",
      " |  lstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  partition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string.  If the separator is found,\n",
      " |      returns a 3-tuple containing the part before the separator, the separator\n",
      " |      itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing the original string\n",
      " |      and two empty strings.\n",
      " |  \n",
      " |  replace(self, old, new, count=-1, /)\n",
      " |      Return a copy with all occurrences of substring old replaced by new.\n",
      " |      \n",
      " |        count\n",
      " |          Maximum number of occurrences to replace.\n",
      " |          -1 (the default value) means replace all occurrences.\n",
      " |      \n",
      " |      If the optional argument count is given, only the first count occurrences are\n",
      " |      replaced.\n",
      " |  \n",
      " |  rfind(...)\n",
      " |      S.rfind(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Return -1 on failure.\n",
      " |  \n",
      " |  rindex(...)\n",
      " |      S.rindex(sub[, start[, end]]) -> int\n",
      " |      \n",
      " |      Return the highest index in S where substring sub is found,\n",
      " |      such that sub is contained within S[start:end].  Optional\n",
      " |      arguments start and end are interpreted as in slice notation.\n",
      " |      \n",
      " |      Raises ValueError when the substring is not found.\n",
      " |  \n",
      " |  rjust(self, width, fillchar=' ', /)\n",
      " |      Return a right-justified string of length width.\n",
      " |      \n",
      " |      Padding is done using the specified fill character (default is a space).\n",
      " |  \n",
      " |  rpartition(self, sep, /)\n",
      " |      Partition the string into three parts using the given separator.\n",
      " |      \n",
      " |      This will search for the separator in the string, starting at the end. If\n",
      " |      the separator is found, returns a 3-tuple containing the part before the\n",
      " |      separator, the separator itself, and the part after it.\n",
      " |      \n",
      " |      If the separator is not found, returns a 3-tuple containing two empty strings\n",
      " |      and the original string.\n",
      " |  \n",
      " |  rsplit(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the words in the string, using sep as the delimiter string.\n",
      " |      \n",
      " |        sep\n",
      " |          The delimiter according which to split the string.\n",
      " |          None (the default value) means split according to any whitespace,\n",
      " |          and discard empty strings from the result.\n",
      " |        maxsplit\n",
      " |          Maximum number of splits to do.\n",
      " |          -1 (the default value) means no limit.\n",
      " |      \n",
      " |      Splits are done starting at the end of the string and working to the front.\n",
      " |  \n",
      " |  rstrip(self, chars=None, /)\n",
      " |      Return a copy of the string with trailing whitespace removed.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  split(self, /, sep=None, maxsplit=-1)\n",
      " |      Return a list of the words in the string, using sep as the delimiter string.\n",
      " |      \n",
      " |      sep\n",
      " |        The delimiter according which to split the string.\n",
      " |        None (the default value) means split according to any whitespace,\n",
      " |        and discard empty strings from the result.\n",
      " |      maxsplit\n",
      " |        Maximum number of splits to do.\n",
      " |        -1 (the default value) means no limit.\n",
      " |  \n",
      " |  splitlines(self, /, keepends=False)\n",
      " |      Return a list of the lines in the string, breaking at line boundaries.\n",
      " |      \n",
      " |      Line breaks are not included in the resulting list unless keepends is given and\n",
      " |      true.\n",
      " |  \n",
      " |  startswith(...)\n",
      " |      S.startswith(prefix[, start[, end]]) -> bool\n",
      " |      \n",
      " |      Return True if S starts with the specified prefix, False otherwise.\n",
      " |      With optional start, test S beginning at that position.\n",
      " |      With optional end, stop comparing S at that position.\n",
      " |      prefix can also be a tuple of strings to try.\n",
      " |  \n",
      " |  strip(self, chars=None, /)\n",
      " |      Return a copy of the string with leading and trailing whitespace remove.\n",
      " |      \n",
      " |      If chars is given and not None, remove characters in chars instead.\n",
      " |  \n",
      " |  swapcase(self, /)\n",
      " |      Convert uppercase characters to lowercase and lowercase characters to uppercase.\n",
      " |  \n",
      " |  title(self, /)\n",
      " |      Return a version of the string where each word is titlecased.\n",
      " |      \n",
      " |      More specifically, words start with uppercased characters and all remaining\n",
      " |      cased characters have lower case.\n",
      " |  \n",
      " |  translate(self, table, /)\n",
      " |      Replace each character in the string using the given translation table.\n",
      " |      \n",
      " |        table\n",
      " |          Translation table, which must be a mapping of Unicode ordinals to\n",
      " |          Unicode ordinals, strings, or None.\n",
      " |      \n",
      " |      The table must implement lookup/indexing via __getitem__, for instance a\n",
      " |      dictionary or list.  If this operation raises LookupError, the character is\n",
      " |      left untouched.  Characters mapped to None are deleted.\n",
      " |  \n",
      " |  upper(self, /)\n",
      " |      Return a copy of the string converted to uppercase.\n",
      " |  \n",
      " |  zfill(self, width, /)\n",
      " |      Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
      " |      \n",
      " |      The string is never truncated.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  maketrans(x, y=None, z=None, /)\n",
      " |      Return a translation table usable for str.translate().\n",
      " |      \n",
      " |      If there is only one argument, it must be a dictionary mapping Unicode\n",
      " |      ordinals (integers) or characters to Unicode ordinals, strings or None.\n",
      " |      Character keys will be then converted to ordinals.\n",
      " |      If there are two arguments, they must be strings of equal length, and\n",
      " |      in the resulting dictionary, each character in x will be mapped to the\n",
      " |      character at the same position in y. If there is a third argument, it\n",
      " |      must be a string, whose characters will be mapped to None in the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模式匹配与正则表达式\n",
    "\n",
    "正则表达式介绍见[笔记](https://libertydream.github.io/python_tutorial/7.regular_expression_and_JSON.html)，在线验证学习网站[regexr](https://regexr.com)，进阶练习[Regex Golf](https://alf.nu/RegexGolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_text = 'one1two2three3four4five5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(\\d+)',re_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'(\\D+)',re_text).group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two', 'three', 'four', 'five', '']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\d',re_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1one2two3three4four5five'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'(\\D+)(\\d+)',r'\\2\\1', re_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 One 2 Two 3 Three 4 Four 5 Five '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sub_func(m):\n",
    "    return m.group(2) + ' '+ m.group(1).title()+\" \"\n",
    "\n",
    "re.sub(r'(\\D+)(\\d+)',sub_func, re_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本处理与NLTK\n",
    "\n",
    "[NLTK](https://www.nltk.org/)，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库。\n",
    "\n",
    "## 1. 分词（Tokenizaiton）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/bible.txt','r') as f:\n",
    "    texts = f.read()\n",
    "\n",
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 断句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:1 In the beginning God created the heaven and the earth.',\n",
       " '1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep.',\n",
       " 'And the Spirit of God moved upon the face of the\\nwaters.',\n",
       " '1:3 And God said, Let there be light: and there was light.',\n",
       " '1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.',\n",
       " '1:5 And God called the light Day, and the darkness he called Night.',\n",
       " 'And the evening and the morning were the first day.',\n",
       " '1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.',\n",
       " '1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.',\n",
       " '1:8 And God called the firmament Heaven.',\n",
       " 'And the evening and the\\nmorning were the second day.',\n",
       " '1:9 And God said, Let the waters under the heaven be gathered together\\nunto one place, and let the dry land appear: and it was so.',\n",
       " '1:10 And God called the dry land Earth; and the gathering together of\\nthe waters called he Seas: and God saw that it was good.',\n",
       " '1:11 And God said, Let the earth bring forth grass, the herb yielding\\nseed, and the fruit tree yielding fruit after his kind, whose seed is\\nin itself, upon the earth: and it was so.',\n",
       " '1:12 And the earth brought forth grass, and herb yielding seed after\\nhis kind, and the tree yielding fruit, whose seed was in itself, after\\nhis kind: and God saw that it was good.',\n",
       " '1:13 And the evening and the morning were the third day.',\n",
       " '1:14 And God said, Let there be lights in the firmament of the heaven\\nto divide the day from the night; and let them be for signs, and for\\nseasons, and for days, and years: 1:15 And let them be for lights in\\nthe firmament of the heaven to give light upon the earth: and it was\\nso.',\n",
       " '1:16 And God made two great lights; the greater light to rule the day,\\nand the lesser light to rule the night: he made the stars also.',\n",
       " '1:17 And God set them in the firmament of the heaven to give light\\nupon the earth, 1:18 And to rule over the day and over the night, and\\nto divide the light from the darkness: and God saw that it was good.',\n",
       " '1:19 And the evening and the morning were the fourth day.',\n",
       " '1:20 And God said, Let the waters bring forth abundantly the moving\\ncreature that hath life, and fowl that may fly above the earth in the\\nopen firmament of heaven.',\n",
       " '1:21 And God created great whales, and every living creature that\\nmoveth, which the waters brought forth abundantly, after their kind,\\nand every winged fowl after his kind: and God saw that it was good.',\n",
       " '1:22 And God blessed them, saying, Be fruitful, and multiply, and fill\\nthe waters in the seas, and let fowl multiply in the earth.',\n",
       " '1:23 And the evening and the morning were the fifth day.',\n",
       " '1:24 And God said, Let the earth bring forth the living creature after\\nhis kind, cattle, and creeping thing, and beast of the earth after his\\nkind: and it was so.',\n",
       " '1:25 And God made the beast of the earth after his kind, and cattle\\nafter their kind, and every thing that creepeth upon the earth after\\nhis kind: and God saw that it was good.',\n",
       " '1:26 And God said, Let us make man in our image, after our likeness:\\nand let them have dominion over the fish of the sea, and over the fowl\\nof the air, and over the cattle, and over all the earth, and over\\nevery creeping thing that creepeth upon the earth.',\n",
       " '1:27 So God created man in his own image, in the image of God created\\nhe him; male and female created he them.',\n",
       " '1:28 And God blessed them, and God said unto them, Be fruitful, and\\nmultiply, and replenish the earth, and subdue it: and have dominion\\nover the fish of the sea, and over the fowl of the air, and over every\\nliving thing that moveth upon the earth.',\n",
       " '1:29 And God said, Behold, I have given you every herb bearing seed,\\nwhich is upon the face of all the earth, and every tree, in the which\\nis the fruit of a tree yielding seed; to you it shall be for meat.',\n",
       " '1:30 And to every beast of the earth, and to every fowl of the air,\\nand to every thing that creepeth upon the earth, wherein there is\\nlife, I have given every green herb for meat: and it was so.',\n",
       " '1:31 And God saw every thing that he had made, and, behold, it was\\nvery good.',\n",
       " 'And the evening and the morning were the sixth day.',\n",
       " '2:1 Thus the heavens and the earth were finished, and all the host of\\nthem.',\n",
       " '2:2 And on the seventh day God ended his work which he had made; and\\nhe rested on the seventh day from all his work which he had made.',\n",
       " '2:3 And God blessed the seventh day, and sanctified it: because that\\nin it he had rested from all his work which God created and made.',\n",
       " '2:4 These are the generations of the heavens and of the earth when\\nthey were created, in the day that the LORD God made the earth and the\\nheavens, 2:5 And every plant of the field before it was in the earth,\\nand every herb of the field before it grew: for the LORD God had not\\ncaused it to rain upon the earth, and there was not a man to till the\\nground.',\n",
       " '2:6 But there went up a mist from the earth, and watered the whole\\nface of the ground.',\n",
       " '2:7 And the LORD God formed man of the dust of the ground, and\\nbreathed into his nostrils the breath of life; and man became a living\\nsoul.',\n",
       " '2:8 And the LORD God planted a garden eastward in Eden; and there he\\nput the man whom he had formed.',\n",
       " '2:9 And out of the ground made the LORD God to grow every tree that is\\npleasant to the sight, and good for food; the tree of life also in the\\nmidst of the garden, and the tree of knowledge of good and evil.',\n",
       " '2:10 And a river went out of Eden to water the garden; and from thence\\nit was parted, and became into four heads.',\n",
       " '2:11 The name of the first is Pison: that is it which compasseth the\\nwhole land of Havilah, where there is gold; 2:12 And the gold of that\\nland is good: there is bdellium and the onyx stone.',\n",
       " '2:13 And the name of the second river is Gihon: the same is it that\\ncompasseth the whole land of Ethiopia.',\n",
       " '2:14 And the name of the third river is Hiddekel: that is it which\\ngoeth toward the east of Assyria.',\n",
       " 'And the fourth river is Euphrates.',\n",
       " '2:15 And the LORD God took the man, and put him into the garden of\\nEden to dress it and to keep it.',\n",
       " '2:16 And the LORD God commanded the man, saying, Of every tree of the\\ngarden thou mayest freely eat: 2:17 But of the tree of the knowledge\\nof good and evil, thou shalt not eat of it: for in the day that thou\\neatest thereof thou shalt surely die.',\n",
       " '2:18 And the LORD God said, It is not good that the man should be\\nalone; I will make him an help meet for him.',\n",
       " '2:19 And out of the ground the LORD God formed every beast of the\\nfield, and every fowl of the air; and brought them unto Adam to see\\nwhat he would call them: and whatsoever Adam called every living\\ncreature, that was the name thereof.',\n",
       " '2:20 And Adam gave names to all cattle, and to the fowl of the air,\\nand to every beast of the field; but for Adam there was not found an\\nhelp meet for him.',\n",
       " '2:21 And the LORD God caused a deep sleep to fall upon Adam, and he\\nslept: and he took one of his ribs, and closed up the flesh instead\\nthereof; 2:22 And the rib, which the LORD God had taken from man, made\\nhe a woman, and brought her unto the man.',\n",
       " '2:23 And Adam said, This is now bone of my bones, and flesh of my\\nflesh: she shall be called Woman, because she was taken out of Man.',\n",
       " '2:24 Therefore shall a man leave his father and his mother, and shall\\ncleave unto his wife: and they shall be one flesh.',\n",
       " '2:25 And they were both naked, the man and his wife, and were not\\nashamed.',\n",
       " '3:1 Now the serpent was more subtil than any beast of the field which\\nthe LORD God had made.',\n",
       " 'And he said unto the woman, Yea, hath God said,\\nYe shall not eat of every tree of the garden?',\n",
       " '3:2 And the woman said\\nunto the serpent, We may eat of the fruit of the trees of the garden:\\n3:3 But of the fruit of the tree which is in the midst of the garden,\\nGod hath said, Ye shall not eat of it, neither shall ye touch it, lest\\nye die.',\n",
       " '3:4 And the serpent said unto the woman, Ye shall not surely die: 3:5\\nFor God doth know that in the day ye eat thereof, then your eyes shall\\nbe opened, and ye shall be as gods, knowing good and evil.',\n",
       " '3:6 And when the woman saw that the tree was good for food, and that\\nit was pleasant to the eyes, and a tree to be desired to make one\\nwise, she took of the fruit thereof, and did eat, and gave also unto\\nher husband with her; and he did eat.',\n",
       " '3:7 And the eyes of them both were opened, and they knew that they\\nwere naked; and they sewed fig leaves together, and made themselves\\naprons.',\n",
       " '3:8 And they heard the voice of the LORD God walking in the garden in\\nthe cool of the day: and Adam and his wife hid themselves from the\\npresence of the LORD God amongst the trees of the garden.',\n",
       " '3:9 And the LORD God called unto Adam, and said unto him, Where art\\nthou?',\n",
       " '3:10 And he said, I heard thy voice in the garden, and I was\\nafraid, because I was naked; and I hid myself.',\n",
       " '3:11 And he said, Who told thee that thou wast naked?',\n",
       " 'Hast thou eaten\\nof the tree, whereof I commanded thee that thou shouldest not eat?',\n",
       " '3:12 And the man said, The woman whom thou gavest to be with me, she\\ngave me of the tree, and I did eat.',\n",
       " '3:13 And the LORD God said unto the woman, What is this that thou hast\\ndone?',\n",
       " 'And the woman said, The serpent beguiled me, and I did eat.',\n",
       " '3:14 And the LORD God said unto the serpent, Because thou hast done\\nthis, thou art cursed above all cattle, and above every beast of the\\nfield; upon thy belly shalt thou go, and dust shalt thou eat all the\\ndays of thy life: 3:15 And I will put enmity between thee and the\\nwoman, and between thy seed and her seed; it shall bruise thy head,\\nand thou shalt bruise his heel.',\n",
       " '3:16 Unto the woman he said, I will greatly multiply thy sorrow and\\nthy conception; in sorrow thou shalt bring forth children; and thy\\ndesire shall be to thy husband, and he shall rule over thee.',\n",
       " '3:17 And unto Adam he said, Because thou hast hearkened unto the voice\\nof thy wife, and hast eaten of the tree, of which I commanded thee,\\nsaying, Thou shalt not eat of it: cursed is the ground for thy sake;\\nin sorrow shalt thou eat of it all the days of thy life; 3:18 Thorns\\nalso and thistles shall it bring forth to thee; and thou shalt eat the\\nherb of the field; 3:19 In the sweat of thy face shalt thou eat bread,\\ntill thou return unto the ground; for out of it wast thou taken: for\\ndust thou art, and unto dust shalt thou return.',\n",
       " \"3:20 And Adam called his wife's name Eve; because she was the mother\\nof all living.\",\n",
       " '3:21 Unto Adam also and to his wife did the LORD God make coats of\\nskins, and clothed them.',\n",
       " '3:22 And the LORD God said, Behold, the man is become as one of us, to\\nknow good and evil: and now, lest he put forth his hand, and take also\\nof the tree of life, and eat, and live for ever: 3:23 Therefore the\\nLORD God sent him forth from the garden of Eden, to till the ground\\nfrom whence he was taken.',\n",
       " '3:24 So he drove out the man; and he placed at the east of the garden\\nof Eden Cherubims, and a flaming sword which turned every way, to keep\\nthe way of the tree of life.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(texts)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:1',\n",
       " 'In',\n",
       " 'the',\n",
       " 'beginning',\n",
       " 'God',\n",
       " 'created',\n",
       " 'the',\n",
       " 'heaven',\n",
       " 'and',\n",
       " 'the',\n",
       " 'earth',\n",
       " '.',\n",
       " '1:2',\n",
       " 'And',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'was',\n",
       " 'without',\n",
       " 'form',\n",
       " ',']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(texts)\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 停用词\n",
    "\n",
    "在自然语言处理的很多任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对统计方法带来一些干扰，我们把这类词叫做**停用词**，通常我们会用一个停用词表把它们过滤出来。比如英语当中的定冠词/不定冠词（a，an，the等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1:1',\n",
       " 'In',\n",
       " 'beginning',\n",
       " 'God',\n",
       " 'created',\n",
       " 'heaven',\n",
       " 'earth',\n",
       " '.',\n",
       " '1:2',\n",
       " 'And',\n",
       " 'earth',\n",
       " 'without',\n",
       " 'form',\n",
       " ',',\n",
       " 'void',\n",
       " ';',\n",
       " 'darkness',\n",
       " 'upon',\n",
       " 'face',\n",
       " 'deep']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [w for w in words if not w in stop_words]\n",
    "filtered_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words) - len(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 词性标注\n",
    "\n",
    "词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。\n",
    "\n",
    "词性标注（part-of-speech tagging），又称为词类标注，确定每个词是名词、动词、形容词或者其他词性的过程。\n",
    "\n",
    "词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。词性标注的最简单做法是选取最高频词性，主流的做法可以分为基于规则和基于统计的方法，包括：\n",
    "\n",
    "- 基于最大熵的词性标注\n",
    "- 基于统计最大概率输出词性\n",
    "- 基于HMM的词性标注\n",
    "\n",
    "![](https://raw.githubusercontent.com/LibertyDream/diy_img_host/master/img/2019-12-25_part_of_speech_tagset.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1:1', 'CD'),\n",
       " ('In', 'IN'),\n",
       " ('beginning', 'VBG'),\n",
       " ('God', 'NNP'),\n",
       " ('created', 'VBD'),\n",
       " ('heaven', 'RB'),\n",
       " ('earth', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('1:2', 'CD'),\n",
       " ('And', 'CC'),\n",
       " ('earth', 'NN'),\n",
       " ('without', 'IN'),\n",
       " ('form', 'NN'),\n",
       " (',', ','),\n",
       " ('void', 'NN'),\n",
       " (';', ':'),\n",
       " ('darkness', 'CC'),\n",
       " ('upon', 'IN'),\n",
       " ('face', 'NN'),\n",
       " ('deep', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "tags = pos_tag(filtered_words)\n",
    "tags[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 组块分析（chunking）\n",
    "\n",
    "分块是命名实体识别的基础，词性给出来的句子成分的属性，但有时候，更多的信息（比如句子句法结构）可以帮助我们对句子中的模式挖掘更充分。举个例子，”古天乐赞助了很多小学“中的头部古天乐是一个人名（命名实体）\n",
    "\n",
    "组块分析是一个非常有用的从文本抽取信息的方法，提取组块需要用到正则表达式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"\"\"\n",
    "    NP:{<JJ>*<NN>+}\n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "he National Wrestling Association was an early professional wrestling sanctioning body created in 1930 by \n",
    "the National Boxing Association (NBA)(now the World Boxing Association, WBA) as an attempt to create a governing \n",
    "body for professional wrestling in the United States. The group created a number of \"world\"level championships \n",
    "as an attempt to klear up the professional wrestling rankings which at the time saw a number of different championships \n",
    "promoted as the \"true world championship\". The National Wrestling Association's NWA World Heavyweight Championship was \n",
    "later considered part of the historical lineage of the National Wrestling Alliance's NWA World Heavyweight Championship \n",
    "when then National Wrestling Association champion Lou Thesz won the National Wrestling Alliance championship, folding \n",
    "the original championship into one title in 1949.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = sent_tokenize(text)\n",
    "\n",
    "tokenized_words = [word_tokenize(sent) for sent in tokenized_sent]\n",
    "\n",
    "tagged_words = [pos_tag(word) for word in tokenized_words]\n",
    "\n",
    "word_tree = [chunker.parse(word) for word in tagged_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tree[0].draw() # 弹窗显示解析图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 命名实体识别\n",
    "\n",
    "命名实体识别（Named Entity Recognition，简称NER），识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。通常包括两部分：1）实体边界识别；2）确定实体类别（人名、地名、机构名或其他）。\n",
    "\n",
    "就 NER 而言，NLTK 不太好用，可以用斯坦福的[模块](https://stanfordnlp.github.io/CoreNLP)。更快，识别更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Tom/NNP)\n",
      "  studies/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION Oxford/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Tom studies at Oxford University.'\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming和Lemmatizing\n",
    "\n",
    "很多时候我们需要对英文当中的时态语态等做归一化，这个时候我们就需要stemming和lemmatizing这样的操作了。比如“running“是进行时，但是这个词表征的含义和“run“是一致的，我们在识别语义的时候，希望能消除这种差异化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('makes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('swimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer('english')\n",
    "stemmer2.stem('growing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('makes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. WordNet 与词义解析\n",
    "\n",
    "各种“2Vec”模型和 FastText 之前，有一种表示词的方法就是编字典，即规则式的解析词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('serviceman.n.01'),\n",
       " Synset('man.n.03'),\n",
       " Synset('homo.n.02'),\n",
       " Synset('man.n.05'),\n",
       " Synset('man.n.06'),\n",
       " Synset('valet.n.01'),\n",
       " Synset('man.n.08'),\n",
       " Synset('man.n.09'),\n",
       " Synset('man.n.10'),\n",
       " Synset('world.n.08'),\n",
       " Synset('man.v.01'),\n",
       " Synset('man.v.02')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.synsets('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'someone who serves in the armed forces; a member of a military force'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('man')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barked all night'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "dog.examples()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上位词。“狗”属于“犬类”\n",
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英文文本处理与spaCy\n",
    "\n",
    "[spaCy](https://spacy.io/) 是 Python 和 Cython 中的高级自然语言处理库，它建立在最新的研究基础之上，从一开始就设计用于实际产品。spaCy带有预先训练的统计模型和单词向量，目前支持20多种语言的标记。它具有世界上速度最快的句法分析器，用于标签的卷积神经网络模型，解析和命名实体识别以及与深度学习整合。\n",
    "\n",
    "## 1.Tokenization\n",
    "\n",
    "文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。在spaCY中同样可以很方便地完成Tokenization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello\"\n",
      "\"everybody\"\n",
      "\"!\"\n",
      "\"My\"\n",
      "\"name\"\n",
      "\"is\"\n",
      "\"Daniel\"\n",
      "\"Meng\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp =  spacy.load('en_core_web_sm')\n",
    "doc = nlp('Hello everybody! My name is Daniel Meng')\n",
    "\n",
    "for token in doc:\n",
    "    print('\"'+token.text+'\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next\t0\tnext\tFalse\tFalse\tXxxx\tADJ\tJJ\n",
      "week\t5\tweek\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "I\t10\t-PRON-\tFalse\tFalse\tX\tPRON\tPRP\n",
      "'ll\t11\twill\tFalse\tFalse\t'xx\tAUX\tMD\n",
      "be\t15\tbe\tFalse\tFalse\txx\tAUX\tVB\n",
      "in\t18\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "Shanghai\t21\tShanghai\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
      ".\t29\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week I'll be in Shanghai.\")\n",
    "for token in doc: \n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text, \n",
    "        token.idx, \n",
    "        token.lemma_, \n",
    "        token.is_punct, \n",
    "        token.is_space, \n",
    "        token.shape_, \n",
    "        token.pos_, \n",
    "        token.tag_\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc 对象内部也包含了断句功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everybody!\n",
      "My name is Daniel Meng\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello everybody! My name is Daniel Meng')\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'UH'), ('everybody', 'NN'), ('!', '.'), ('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Daniel', 'NNP'), ('Meng', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello everybody! My name is Daniel Meng')\n",
    "print([(token.text, token.tag_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.命名实体识别\n",
    "\n",
    "相较于 NLTK，spaCy 做 NER 简单且可标注类别更多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next week DATE\n",
      "Shanghai GPE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Next week I'll be in Shanghai.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 CARDINAL\n",
      "9 a.m. TIME\n",
      "30% PERCENT\n",
      "just 2 days DATE\n",
      "WSJ ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy 做可视化也不错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I just bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    9 a.m.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " because the stock went up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    30%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    just 2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    WSJ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"I just bought 2 shares at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ\")\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.chunking\n",
    "\n",
    "spaCy 能自动检测名词短语，并输出词根"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal NP Journal\n",
      "an interesting piece NP piece\n",
      "crypto currencies NP currencies\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks: \n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.句法依存解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP<--compound--Street/NNP\n",
      "Street/NNP<--compound--Journal/NNP\n",
      "Journal/NNP<--nsubj--published/VBD\n",
      "just/RB<--advmod--published/VBD\n",
      "published/VBD<--ROOT--published/VBD\n",
      "an/DT<--det--piece/NN\n",
      "interesting/JJ<--amod--piece/NN\n",
      "piece/NN<--dobj--published/VBD\n",
      "on/IN<--prep--piece/NN\n",
      "crypto/NN<--compound--currencies/NNS\n",
      "currencies/NNS<--pobj--on/IN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "\n",
    "for token in doc:\n",
    "    print('{0}/{1}<--{2}--{3}/{4}'.format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"000519d3e0174f93aff70976876cd653-0\" class=\"displacy\" width=\"1040\" height=\"272.0\" direction=\"ltr\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wall</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">Street</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Journal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">just</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">published</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">interesting</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">piece</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">crypto</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">currencies</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-6\" stroke-width=\"2px\" d=\"M430,137.0 C430,2.0 680.0,2.0 680.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M680.0,139.0 L688.0,127.0 672.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-7\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-000519d3e0174f93aff70976876cd653-0-9\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-000519d3e0174f93aff70976876cd653-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945.0,139.0 L953.0,127.0 937.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.词向量\n",
    "\n",
    "word2vec 是 NLP 中最常用的文本表示方法，将词用稠密低维向量表示，相似的词在向量空间中也离得比较近。具体可以看我[博客](https://libertydream.github.io/2019/11/09/%E5%9B%BE%E8%A7%A3-word2vec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['banana'].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "cosine_sim = lambda x, y: 1-spatial.distance.cosine(x,y)\n",
    "\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    "\n",
    "# 向量加减运算\n",
    "maybe_king = man - woman + queen\n",
    "computed_sim = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    "    \n",
    "    similarity = cosine_sim(maybe_king, word.vector)\n",
    "    computed_sim.append((word, similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Queen', 'QUEEN', 'queen', 'King', 'KING', 'king', 'KIng', 'Kings', 'KINGS', 'kings']\n"
     ]
    }
   ],
   "source": [
    "# 排序显示最接近的词向量\n",
    "computed_sim = sorted(computed_sim, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_sim[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.词汇与文本相似度\n",
    "\n",
    "在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66185343 0.2355285\n"
     ]
    }
   ],
   "source": [
    "banana = nlp.vocab['banana']\n",
    "dog = nlp.vocab['dog']\n",
    "fruit = nlp.vocab['fruit']\n",
    "animal = nlp.vocab['animal']\n",
    "\n",
    "# 狗-动物 相似度，狗-水果 相似度\n",
    "print(dog.similarity(animal), dog.similarity(fruit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67148364 0.24272852\n"
     ]
    }
   ],
   "source": [
    "print(banana.similarity(fruit), banana.similarity(animal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以粗略的看一下语义相似度，即文本间的关联程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8901765218466683\n",
      "0.9115828449161616\n",
      "0.8588322588373283\n"
     ]
    }
   ],
   "source": [
    "target = nlp('Cats are beautiful animals.')\n",
    "\n",
    "cmp_dog = nlp('Dogs are awesome.')\n",
    "cmp_str = nlp('Some gorgeous creatures are felines.')\n",
    "cmp_dolphin = nlp('Dolphins are swimming animals.')\n",
    "\n",
    "print(target.similarity(cmp_dog))\n",
    "print(target.similarity(cmp_str))\n",
    "print(target.similarity(cmp_dolphin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中文文本处理与jieba\n",
    "\n",
    "## 1.分词\n",
    "\n",
    "对于中文和日文这样的特殊亚洲语系文本而言，字和字之间是紧密相连的，单纯从文本形态上无法区分具备独立含义的词（拉丁语系纯天然由空格分隔不同的word），而不同的询以不同的方式排布，可以表达不同的内容和情感。\n",
    "\n",
    "> Transformer 之后基本不做分词了，当做隐因子自动学习\n",
    "\n",
    "目前主流的分词方法主要是**基于词典匹配的分词方法（正向最大匹配法、逆向最大匹配法和双向匹配分词法等）和基于统计的分词方法（HMM、CRF、和深度学习）**；主流的分词工具库包括中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库等。\n",
    "\n",
    "这里简单介绍 [jieba](https://github.com/fxsjy/jieba) 的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全模式:  我/ 在/ 网络/ 网络平台/ 平台/ 台上/ 分享/ 自然/ 自然语言/ 语言/ 处理/ 知识\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut('我在网络平台上分享自然语言处理知识', cut_all=True)\n",
    "print('全模式:  ' + '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精确模式（默认）:  我/ 在/ 网络平台/ 上/ 分享/ 自然语言/ 处理/ 知识\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut('我在网络平台上分享自然语言处理知识', cut_all=False)\n",
    "print('精确模式（默认）:  ' + '/ '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '在', '网络平台', '上', '分享', '自然语言', '处理', '知识']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回 list\n",
    "jieba.lcut('我在网络平台上分享自然语言处理知识')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有种适合建立倒排表的方法 `cut_for_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "老王, 硕士, 毕业, 于, 清华, 华大, 大学, 清华大学, ，, 后, 前往, 福大, 大学, 斯坦福, 斯坦福大学, 深造\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut_for_search('老王硕士毕业于清华大学，后前往斯坦福大学深造')\n",
    "print(', '.join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['老王',\n",
       " '硕士',\n",
       " '毕业',\n",
       " '于',\n",
       " '清华',\n",
       " '华大',\n",
       " '大学',\n",
       " '清华大学',\n",
       " '，',\n",
       " '后',\n",
       " '前往',\n",
       " '福大',\n",
       " '大学',\n",
       " '斯坦福',\n",
       " '斯坦福大学',\n",
       " '深造']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.lcut_for_search('老王硕士毕业于清华大学，后前往斯坦福大学深造')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**添加自定义字典**\n",
    "\n",
    "很多时候我们需要针对自己的场景进行分词，会有一些领域内的专有词汇。\n",
    "\n",
    "1. 可以用jieba.load_userdictfile_name）加载用户字典\n",
    "2. 少量的词汇可以自己用下面方法手动添加：\n",
    "    - 用add_word（word，freq=None，tag=None）和del_word（word）在程序中动态修改词典\n",
    "    - 用suggest_freq（segment，tune=True）可调节单个词语的词频，使其能（或不能）被分出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'如果/放到/旧/字典/中将/出错/。'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'如果/放到/旧/字典/中/将/出错/。'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq(('中','将'),True)\n",
    "'/'.join(jieba.cut('如果放到旧字典中将出错。', HMM=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.jieba词性标注\n",
    "\n",
    "| tag  | 成分     | 描述                                                         |\n",
    "| ---- | -------- | ------------------------------------------------------------ |\n",
    "| ag   | 形语素   | 形容词性语素。形容词代码为 a，语素代码ｇ前面置以A。          |\n",
    "| a    | 形容词   | 取英语形容词 adjective的第1个字母。                          |\n",
    "| ad   | 副形词   | 直接作状语的形容词。形容词代码 a和副词代码d并在一起。        |\n",
    "| an   | 名形词   | 具有名词功能的形容词。形容词代码 a和名词代码n并在一起。      |\n",
    "| b    | 区别词   | 取汉字“别”的声母。                                           |\n",
    "| c    | 连词     | 取英语连词 conjunction的第1个字母。                          |\n",
    "| dg   | 副语素   | 副词性语素。副词代码为 d，语素代码ｇ前面置以D。              |\n",
    "| d    | 副词     | 取 adverb的第2个字母，因其第1个字母已用于形容词。            |\n",
    "| e    | 叹词     | 取英语叹词 exclamation的第1个字母。                          |\n",
    "| f    | 方位词   | 取汉字“方”                                                   |\n",
    "| g    | 语素     | 绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。       |\n",
    "| h    | 前接成分 | 取英语 head的第1个字母。                                     |\n",
    "| i    | 成语     | 取英语成语 idiom的第1个字母。                                |\n",
    "| j    | 简称略语 | 取汉字“简”的声母。                                           |\n",
    "| k    | 后接成分 |                                                              |\n",
    "| l    | 习用语   | 习用语尚未成为成语，有点“临时性”，取“临”的声母。             |\n",
    "| m    | 数词     | 取英语 numeral的第3个字母，n，u已有他用。                    |\n",
    "| ng   | 名语素   | 名词性语素。名词代码为 n，语素代码ｇ前面置以N。              |\n",
    "| n    | 名词     | 取英语名词 noun的第1个字母。                                 |\n",
    "| nr   | 人名     | 名词代码 n和“人(ren)”的声母并在一起。                        |\n",
    "| ns   | 地名     | 名词代码 n和处所词代码s并在一起。                            |\n",
    "| nt   | 机构团体 | “团”的声母为 t，名词代码n和t并在一起。                       |\n",
    "| nz   | 其他专名 | “专”的声母的第 1个字母为z，名词代码n和z并在一起。            |\n",
    "| o    | 拟声词   | 取英语拟声词 onomatopoeia的第1个字母。                       |\n",
    "| p    | 介词     | 取英语介词 prepositional的第1个字母。                        |\n",
    "| q    | 量词     | 取英语 quantity的第1个字母。                                 |\n",
    "| r    | 代词     | 取英语代词 pronoun的第2个字母,因p已用于介词。                |\n",
    "| s    | 处所词   | 取英语 space的第1个字母。                                    |\n",
    "| tg   | 时语素   | 时间词性语素。时间词代码为 t,在语素的代码g前面置以T。        |\n",
    "| t    | 时间词   | 取英语 time的第1个字母。                                     |\n",
    "| u    | 助词     | 取英语助词 auxiliary                                         |\n",
    "| vg   | 动语素   | 动词性语素。动词代码为 v。在语素的代码g前面置以V。           |\n",
    "| v    | 动词     | 取英语动词 verb的第一个字母。                                |\n",
    "| vd   | 副动词   | 直接作状语的动词。动词和副词的代码并在一起。                 |\n",
    "| vn   | 名动词   | 指具有名词功能的动词。动词和名词的代码并在一起。             |\n",
    "| w    | 标点符号 |                                                              |\n",
    "| x    | 非语素字 | 非语素字只是一个符号，字母 x通常用于代表未知数、符号。       |\n",
    "| y    | 语气词   | 取汉字“语”的声母。                                           |\n",
    "| z    | 状态词   | 取汉字“状”的声母的前一个字母。                               |\n",
    "| un   | 未知词   | 不可识别词及用户自定义词组。取英文Unkonwn首两个字母。(非北大标准，CSW分词中定义) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "在 p\n",
      "网络平台 n\n",
      "上 f\n",
      "分享 v\n",
      "自然语言 l\n",
      "处理 v\n",
      "知识 v\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "words = pseg.cut('我在网络平台上分享自然语言处理知识')\n",
    "\n",
    "for word, flag in words:\n",
    "    print(\"%s %s\" % (word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.关键词抽取\n",
    "\n",
    "结构化内容这里仅举两个传统无监督方法，TF-IDF 与 textRank 作示例\n",
    "\n",
    "**基于 TF-IDF 算法的关键词抽取**\n",
    "\n",
    "jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "   - sentence 为待提取的文本\n",
    "   - topK 为返回几个TF-IDF权最大的关键词，默认值为20\n",
    "   - withWeight 为是否一并返回关键词权重值，默认值为False\n",
    "   - allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'无常 一个 没有 然而 因为 时候 知道 先生 现在 我们 大概 记得 什么 那时 自然 虽然 起来 似乎 自己 还有'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "with open('./data/朝花夕拾.txt','r') as f:\n",
    "    texts = f.read()\n",
    "\n",
    "' '.join(analyse.extract_tags(texts, topK=20, withWeight=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jieba.analyse.set_idf_path(file_path)` 可以自定义语料库\n",
    "`jieba.analyse.set_stop_words(file_path)` 可以自定义停用词\n",
    "\n",
    "**基于 TextRank 算法的关键词抽取**\n",
    "\n",
    "- jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))直接使用，接口相同，注意默认过滤词性。\n",
    "- jieba.analyse.TextRank0新建自定义TextRank实例\n",
    "\n",
    "基本思想：\n",
    "\n",
    "  - 将待抽取关键词的文本进行分词\n",
    "  - 以固定窗口大小（默认为5，通过span属性调整），词之间的共现关系，构建图\n",
    "  - 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有 时候 还有 中国 先生 起来 知道 大家 北京 日本 实在 人们 出来 看见 孩子 觉得 听到 不知 记得 不能\n",
      "-------------分割线-----------------\n",
      "中国 先生 时候 北京 日本 大家 人们 孩子 本子 父亲 南京 孝子 有点 古人 老鼠 母亲 婴儿 地方 地面 学者\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(analyse.textrank(texts, topK=20, withWeight=False, allowPOS=('ns','n','vn','v'))))\n",
    "print('-------------分割线-----------------')\n",
    "print(' '.join(analyse.textrank(texts, topK=20, withWeight=False, allowPOS=('ns','n'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "**作者**：Daniel Meng\n",
    "\n",
    "**博客**：[明月轩](https://libertydream.github.io/)\n",
    "\n",
    "**GitHub**: [LibertyDream](https://github.com/LibertyDream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
